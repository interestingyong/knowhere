https://infiniflow.org/docs/dev/benchmark
https://github.com/infiniflow/infinity/blob/main/python/benchmark/README.md
git clone https://github.com/interestingyong/infinity.git
cd infinity/python/benchmark/
python3.10 -m venv venv
source venv/bin/activate
sudo apt-get update
sudo apt-get install -y pkg-config libhdf5-dev python3-dev
pip install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install --no-cache-dir --no-binary=h5py h5py -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

 aria2c -x 8 http://ann-benchmarks.com/sift-128-euclidean.hdf5
aria2c -x 8 http://ann-benchmarks.com/gist-960-euclidean.hdf5
aria2c -x 8 https://home.apache.org/~mikemccand/enwiki-20120502-lines-1k.txt.lzma  å¤±æ•ˆï¼Œé€šè¿‡lucenutilä»“åº“è„šæœ¬ä¸‹è½½ï¼Œæ”¾åˆ°amazons3äº†ã€‚
export http_proxy=http://192.168.1.202:8889;git clone https://github.com/mikemccand/luceneutil.git
 python src/python/initial_setup.py -download  
create data directory at /home/ictrek/workspace-docker/wy/dataset/data
create indices directory at /home/ictrek/workspace-docker/wy/dataset/indices
download https://luceneutil-corpus-files.s3.ca-central-1.amazonaws.com/enwiki-20120502-lines-1k-fixed-utf8-with-random-label.txt.lzma to /home/ictrek/workspace-docker/wy/dataset/data/enwiki-20120502-lines-1k-fixed-utf8-with-random-label.txt.lzma - might take a long time!
 downloading ... 1%, 82.45 MB/6007.44MB, speed 3593.79 KB/ss

aria2c -x 8 ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz -c
aria2c -x 8 ftp://ftp.irisa.fr/local/texmex/corpus/gist.tar.gz -c




export PRE=/home/ictrek/workspace-docker/wy
mkdir -p $PRE/elasticsearch/data
chown -R 1000:0 elasticsearch/data
 chmod -R 770 elasticsearch/data
docker run -d --name elasticsearch --network host -e "discovery.type=single-node" -e "ES_JAVA_OPTS=-Xms16384m -Xmx32000m" -e "xpack.security.enabled=false" -v $PRE/elasticsearch/data:/usr/share/elasticsearch/data elasticsearch:8.13.4
 pip install requests -i https://pypi.tuna.tsinghua.edu.cn/simple

mkdir -p $PRE/qdrant/storage
docker run -d --name qdrant --network host -v $PRE/qdrant/storage:/qdrant/storage qdrant/qdrant:v1.9.2
mkdir -p $PRE/quickwit
docker run -d --name quickwit --network=host -v $PRE/quickwit/qwdata:/quickwit/qwdata quickwit/quickwit:0.8.1 run
mkdir -p $PRE/infinity
docker run -d --name infinity --network=host -v $PRE/infinity:/var/infinity --ulimit nofile=500000:500000 infiniflow/infinity:nightly


 pip  install ./python/infinity_sdk/ -i https://pypi.tuna.tsinghua.edu.cn/simple
cd  /home/ictrek/workspace-docker/wy/infinity/python/benchmark
 cp ../../../dataset/*.hdf5 datasets/
 /home/ictrek/workspace-docker/wy/infinity/python/benchmark/datasets/sift/sift-128-euclidean.hdf5


ä¿®æ”¹infinityçš„æµ‹è¯•ä»£ç ï¼Œè§£æesè¿”å›å€¼æœ‰é—®é¢˜ã€‚
 /home/ictrek/workspace-docker/wy/infinity/python/benchmark/clients/base_client.py
 for i, result in enumerate(results):
392                         ids = []
393                         for item in result[1:]:
394                             doc_id = item[0] if isinstance(item, tuple) else item
395                             if isinstance(doc_id, str):
396                                 try:
397                                     doc_id = int(doc_id)
398                                 except ValueError:
399                                     continue  # è·³è¿‡æ— æ•ˆID
400                             processed_id = ((doc_id >> 32) << 23) + (doc_id & 0xFFFFFFFF)
401                     x        ids.append(processed_id)
402                         precision = (
403                             len(set(ids).intersection(expected_result[i][1:]))
404                             / self.data["topK"]
405                         )
406                         precisions.append(precision)


 python run.py --dataset sift --engine elasticsearch --import
python run.py --query=16 --engine elasticsearch --dataset sift   æŸ¥mean_timeï¼ˆavglatï¼‰ ,mean_precisionsï¼ˆrecallï¼‰
python run.py --dataset sift --engine elasticsearch --query-express=16

mivluséƒ¨ç½²:
vectordbbenché»˜è®¤æ•°æ®é›†çš„åœ°æ–¹
/tmp/vectordb_bench/dataset/cohere/cohere_medium_1m 

Diskann arm
 ./build/apps/utils/fvecs_to_bin  float ../dataset/sift/sift_learn.fvecs ../dataset/sift/sift_learn.fbin
./build/apps/utils/fvecs_to_bin  float ../dataset/sift/sift_query.fvecs ../dataset/sift/sift_query.fbin
 ./build/apps/utils/compute_groundtruth  --data_type float --dist_fn l2 --base_file ../dataset/sift/sift_learn.fbin --query_file  ../dataset/sift/sift_query.fbin --gt_file ../dataset/sift/sift_query_learn_gt100 --K 100

--sift-1m-128dim
 ./build/apps/build_disk_index --data_type float --dist_fn l2 --data_path ../dataset/sift/sift_learn.fbin --index_path_prefix ../dataset/sift/disk_index_sift_learn_R32_L50_A1.2 -R 64 -L150 -B 0.15 -M 4
 --R æ§åˆ¶ Vamana å›¾ä¸­èŠ‚ç‚¹çš„æœ€å¤§è¿æ¥æ•°
--L   æ„å»ºé˜¶æ®µçš„å€™é€‰åˆ—è¡¨å¤§å°ï¼ˆå€¼è¶Šå¤§ï¼Œå›¾è´¨é‡è¶Šé«˜ï¼Œä½†æ„å»ºæ—¶é—´å»¶é•¿ï¼‰èŒƒå›´ï¼š[1, int32_max]ï¼Œé»˜è®¤å€¼ï¼š100
--B æ§åˆ¶æœç´¢æ—¶çš„å†…å­˜åˆ†é…    DRAM budget in GB for searching the index
                                      to set the compressed level for data
                                      while search happens
--M  DRAM budget in GB for building the index
mkdir ../dataset/sift/res
 ./build/apps/search_disk_index  --data_type float --dist_fn l2 --index_path_prefix ../dataset/sift/disk_index_sift_learn_R32_L50_A1.2 --query_file ../dataset/sift/sift_query.fbin  --gt_file ../dataset/sift/sift_query_learn_gt100 -K 10 -L 150 --result_path ../dataset/sift/res --num_nodes_to_cache 10000
--cohere-1m-768dim
 ./build/apps/build_disk_index --data_type float --dist_fn l2 --data_path ../dataset/cohere/cohere_learn.fbin --index_path_prefix ../dataset/cohere/disk_index_sift_learn_R64_L150_A1.2 -R 64 -L150 -B 0.15 -M 4
 ./build/apps/search_disk_index  --data_type float --dist_fn l2 --index_path_prefix ../dataset/cohere/disk_index_sift_learn_R32_L50_A1.2 --query_file ../dataset/cohere/cohere_query.fbin  --gt_file ../dataset/cohere/cohere_query_learn_gt100 -K 10 -L 150 --result_path ../dataset/cohere/res --num_nodes_to_cache 10000
 ./build/apps/search_disk_index  --data_type float --dist_fn l2 --index_path_prefix ../dataset/cohere/disk_index_sift_learn_R32_L50_A1.2 --query_file ../dataset/cohere/cohere_query.fbin  --gt_file ../dataset/cohere/cohere_query_learn_gt100 -K 10 -L 150 --result_path ../dataset/cohere/res --num_nodes_to_cache 10000 -T 16 (é»˜è®¤æ˜¯8çº¿ç¨‹)

pipeann arm
- åˆ©ç”¨diskannçš„æ–‡ä»¶,uint8/floatéƒ½å¤±è´¥äº†
 export INDEX_PREFIX=/home/ictrek/workspace-docker/wy/dataset/cohere/disk_index_pipe_learn_R64_L150_A1.2
 export DATA_PATH=/home/ictrek/workspace-docker/wy/dataset/cohere/cohere_learn.fbin
 ./build/apps/build_disk_index --data_type float --dist_fn l2 --data_path ../dataset/cohere/cohere_learn.fbin --index_path_prefix ../dataset/cohere/disk_index_pipe_learn_R64_L150_A1.2 -R 64 -L150 -B 0.15 -M 4
 ./build/tests/utils/gen_random_slice float ${DATA_PATH} ${INDEX_PREFIX}_SAMPLE_RATE_0.01 0.01
./build/tests/build_memory_index float ${INDEX_PREFIX}_SAMPLE_RATE_0.01_data.bin ${INDEX_PREFIX}_SAMPLE_RATE_0.01_ids.bin ${INDEX_PREFIX}_mem.index 0 1 64 150 1.2 24 l2
./build/tests/search_disk_index float ${INDEX_PREFIX} 1 32 /home/ictrek/workspace-docker/wy/dataset/cohere/cohere_query.fbin /home/ictrek/workspace-docker/wy/dataset/cohere/cohere_query_learn_gt100 10 l2 2 10 10 20 30 40
./build/tests/search_disk_index float ${INDEX_PREFIX} 1 32 /home/ictrek/workspace-docker/wy/dataset/cohere/cohere_query.fbin /home/ictrek/workspace-docker/wy/dataset/cohere/cohere_query_learn_gt100 10 l2 2 10 150
[å›¾ç‰‡]
- é‡æ–°ç”Ÿæˆçš„æ–‡ä»¶
 ./build/apps/build_disk_index --data_type uint8 --dist_fn l2 --data_path ../dataset/cohere/cohere_learn.fbin --index_path_prefix ../dataset/cohere/disk_index_pipe_learn_R64_L150_A1.2 -R 64 -L150 -B 0.15 -M 4
 export INDEX_PREFIX=/home/ictrek/workspace-docker/wy/dataset/cohere/disk_index_pipe_learn_R64_L150_A1.2
 export DATA_PATH=/home/ictrek/workspace-docker/wy/dataset/cohere/cohere_learn.fbin
 ./build/tests/utils/gen_random_slice uint8 ${DATA_PATH} ${INDEX_PREFIX}_SAMPLE_RATE_0.01 0.01
./build/tests/build_memory_index uint8  ${INDEX_PREFIX}_SAMPLE_RATE_0.01_data.bin ${INDEX_PREFIX}_SAMPLE_RATE_0.01_ids.bin ${INDEX_PREFIX}_mem.index 0 1 64 150 1.2 24 l2
./build/tests/search_disk_index uint8 ${INDEX_PREFIX} 1 32 /home/ictrek/workspace-docker/wy/dataset/cohere/cohere_query.fbin /home/ictrek/workspace-docker/wy/dataset/cohere/cohere_query_learn_gt100 10 l2 2 10 150
[å›¾ç‰‡]
- æµ‹è¯•siftçš„æ•°æ®é›†
 ./build/apps/build_disk_index --data_type uint8 --dist_fn l2 --data_path ../dataset/sift/sift_learn.fbin --index_path_prefix ../dataset/sift/disk_index_sift_learn_R64_L150_A1.2 -R 64 -L150 -B 0.15 -M 4 ï¼ˆDiskANN/build_disk_indexï¼‰
 export INDEX_PREFIX=/home/ictrek/workspace-docker/wy/dataset/sift/disk_index_sift_learn_R64_L150_A1.2
 export DATA_PATH=/home/ictrek/workspace-docker/wy/dataset/sift/sift_learn.fbin
 ./build/tests/utils/gen_random_slice uint8 ${DATA_PATH} ${INDEX_PREFIX}_SAMPLE_RATE_0.01 0.01
./build/tests/build_memory_index uint8  ${INDEX_PREFIX}_SAMPLE_RATE_0.01_data.bin ${INDEX_PREFIX}_SAMPLE_RATE_0.01_ids.bin ${INDEX_PREFIX}_mem.index 0 1 64 150 1.2 24 l2
./build/tests/search_disk_index uint8 ${INDEX_PREFIX} 1 32 /home/ictrek/workspace-docker/wy/dataset/sift/sift_query.fbin /home/ictrek/workspace-docker/wy/dataset/sift/sift_query_learn_gt100 10 l2 2 10 150
[å›¾ç‰‡]
- é‡æ–°åˆå§‹åŒ–æµ‹è¯•
Usage:
ï¼ˆPipeANN/build_disk_indexï¼‰
build/tests/build_disk_index uint8  ../dataset/sift/sift_learn.fbin  ../dataset/sift/sift_learn.fbin 96 128 3.3 256 112 l2 0  
export INDEX_PREFIX=/home/ictrek/workspace-docker/wy/dataset/sift/sift_learn.fbin
build/tests/utils/gen_random_slice uint8  /home/ictrek/workspace-docker/wy/dataset/sift/sift_learn.fbin ${INDEX_PREFIX}_SAMPLE_RATE_0.01 0.01
build/tests/build_memory_index uint8 ${INDEX_PREFIX}_SAMPLE_RATE_0.01_data.bin ${INDEX_PREFIX}_SAMPLE_RATE_0.01_ids.bin ${INDEX_PREFIX}_mem.index 0 0 32 64 1.2 24 l2
build/tests/utils/compute_groundtruth uint8 ../dataset/sift/sift_learn.fbin  / ../dataset/sift/sift_query.fbin  1000 ../dataset/sift/sift_query_learn_gt100

./build/tests/search_disk_index uint8 ${INDEX_PREFIX} 1 32 /home/ictrek/workspace-docker/wy/dataset/sift/sift_query.fbin /home/ictrek/workspace-docker/wy/dataset/sift/sift_query_learn_gt100 100 l2 2 10 150  (topk=100)


build/tests/build_disk_index uint8  ../dataset/cohere/cohere_learn.fbin  ../dataset/cohere/cohere_learn.fbin 96 128 3.3 256 112 l2 0  ï¼ˆPipeANN/build_disk_indexï¼‰
export INDEX_PREFIX=/home/ictrek/workspace-docker/wy/dataset/cohere/cohere_learn.fbin
build/tests/utils/gen_random_slice uint8  /home/ictrek/workspace-docker/wy/dataset/cohere/cohere_learn.fbin ${INDEX_PREFIX}_SAMPLE_RATE_0.01 0.01
build/tests/build_memory_index uint8 ${INDEX_PREFIX}_SAMPLE_RATE_0.01_data.bin ${INDEX_PREFIX}_SAMPLE_RATE_0.01_ids.bin ${INDEX_PREFIX}_mem.index 0 0 32 64 1.2 24 l2
./build/tests/search_disk_index uint8 ${INDEX_PREFIX} 1 32 /home/ictrek/workspace-docker/wy/dataset/cohere/cohere_query.fbin /home/ictrek/workspace-docker/wy/dataset/cohere/cohere_query_learn_gt100 10 l2 2 10 150

[å›¾ç‰‡]
pipeannä¹‹å‰æµ‹è¯•çš„æ–¹æ³•
 export PATH=$PATH:build/tests/:build/:/home/ictrek/workspace-docker/wy/PipeANN/build/:/home/ictrek/workspace-docker/wy/PipeANN/build/tests/:/home/ictrek/workspace-docker/wy/PipeANN/build/tests/utils/
change_pts uint8 bigann.bin 1000000
mv bigann.bin1000000  1M.bin
 compute_groundtruth uint8 1M.bin bigann_query.bbin 1000 1M_gt.bin   (nearest neigbour =1000)
#Usage: build_disk_index <data_type (float/int8/uint8)>  <data_file.bin> <index_prefix_path> <R>  <L>  <B>  <M>  <T> <similarity metric (cosine/l2) case sensitive>. <single_file_index (0/1)> See README for more information on parameters.
build_disk_index uint8 1M.bin 1m 96 128 3.3 128 112 l2 0  (128: L  the size of search list we maintain during index building)
export INDEX_PREFIX=1m # 
export DATA_PATH=1M.bin
gen_random_slice uint8 ${DATA_PATH} ${INDEX_PREFIX}_SAMPLE_RATE_0.01 0.01
build_memory_index uint8 ${INDEX_PREFIX}_SAMPLE_RATE_0.01_data.bin ${INDEX_PREFIX}_SAMPLE_RATE_0.01_ids.bin ${INDEX_PREFIX}_mem.index 0 0 32 64 1.2 24 l2
#build/tests/search_disk_index <data_type> <index_prefix> <nthreads> <I/O pipeline width (max for PipeANN)> <query file> <truth file> <top-K> <similarity> <search_mode (2 for PipeANN)> <L of in-memory index> <Ls for on-disk index>
search_disk_index uint8 ${INDEX_PREFIX} 1 32 bigann_query.bbin 1M_gt.bin 10 l2 2 10 10 20 30 40
[å›¾ç‰‡]

ä½¿ç”¨bigannæå–1Mçš„æ•°æ®æµ‹è¯•
change_pts uint8 bigann.bin 1000000
mv bigann.bin1000000  1M.bin
 compute_groundtruth uint8 1M.bin bigann_query.bbin 1000 1M_gt.bin
#Usage: build_disk_index <data_type (float/int8/uint8)>  <data_file.bin> <index_prefix_path> <R>  <L>  <B>  <M>  <T> <similarity metric (cosine/l2) case sensitive>. <single_file_index (0/1)> See README for more information on parameters.
build_disk_index uint8 1M.bin 1m 96 128 3.3 128 112 l2 0  (128: L  the size of search list we maintain during index building)
export INDEX_PREFIX=1m # on-disk index file name prefix.
export DATA_PATH=1M.bin
gen_random_slice uint8 ${DATA_PATH} ${INDEX_PREFIX}_SAMPLE_RATE_0.01 0.01
build_memory_index uint8 ${INDEX_PREFIX}_SAMPLE_RATE_0.01_data.bin ${INDEX_PREFIX}_SAMPLE_RATE_0.01_ids.bin ${INDEX_PREFIX}_mem.index 0 0 32 64 1.2 24 l2
#build/tests/search_disk_index <data_type> <index_prefix> <nthreads> <I/O pipeline width (max for PipeANN)> <query file> <truth file> <top-K> <similarity> <search_mode (2 for PipeANN)> <L of in-memory index> <Ls for on-disk index>
search_disk_index uint8 ${INDEX_PREFIX} 1 32 bigann_query.bbin 1M_gt.bin 10 l2 2 10 10 20 30 40    éœ€è¦æ³¨æ„10æ˜¯topkï¼Œåé¢çš„10ï¼Œ20ï¼Œ30ï¼Œ40ä»£è¡¨å†…å­˜æœç´¢èŠ‚ç‚¹çš„ä¸ªæ•°ï¼Œä¹Ÿå°±æ˜¯æ·±åº¦memlã€‚ 10ï¼Œ20ï¼Œ30ï¼Œ40æ˜¯4ä¸ªæµ‹è¯•ã€‚ æµ‹è¯•ä»£ç ä¼šè·³è¿‡å°äºtopkçš„memlæµ‹è¯•ã€‚

ä½¿ç”¨bigannæå–100Mçš„æ•°æ®æµ‹è¯•
change_pts uint8 bigann.bin 100000000
mv bigann.bin100000000  100M.bin
 compute_groundtruth uint8 100M.bin bigann_query.bbin 1000 100M_gt.bin (å†…å­˜å ç”¨å¾ˆå¤§ï¼Œæ³¨æ„æ˜¯å¦çœŸçš„å®Œæˆ)
[å›¾ç‰‡]
#Usage: build_disk_index <data_type (float/int8/uint8)>  <data_file.bin> <index_prefix_path> <R>  <L>  <B>  <M>  <T> <similarity metric (cosine/l2) case sensitive>. <single_file_index (0/1)> See README for more information on parameters.
https://github.dev/interestingyong/PipeANN/blob/main/tests/search_disk_index.cpp#L673  
  " B (RAM limit of final index in GB) "  å½±å“PQå¯¹å‘é‡çš„ç¼–ç çš„chunkæ•°é‡ï¼Œ chunkå°‘äº†RAMå°±å°‘äº†ã€‚ calculate_num_pq_chunks
            " M (memory limit while indexing in GB)"    build_merged_vamana_indexè®¡ç®—å†…å­˜ï¼Œå¦‚æœå†…å­˜è¶³å¤Ÿï¼Œè¿˜æ˜¯ä¸€ä¸ªshotï¼Œ å¦åˆ™é€šè¿‡partition_with_ram_budgetè®¡ç®—æ€»å…±éœ€è¦çš„å†…å­˜/é™åˆ¶çš„å†…å­˜ï¼Œå¾—åˆ°åº”è¯¥åˆ‡æˆå‡ ä¸ªåŒºåŸŸæ¥indexã€‚é¿å…å†…å­˜è¿‡è½½ã€‚æœ€æ—©è¿˜æ˜¯ä¼šé€šè¿‡merge_shardsåˆå¹¶ã€‚
             " T (number of threads for indexing) "
build_disk_index uint8 100M.bin 100m 96 128 3.3 128 112 l2 0  (128: L  the size of search list we maintain during index building) e1001æ— æ³•å®Œæˆï¼Œå†…å­˜è¿‡å¤§ã€‚
build_disk_index uint8 100M.bin 100m 96 64 3.3 128 112 l2 0 e1001æ— æ³•å®Œæˆï¼Œå†…å­˜è¿‡å¤§ã€‚
build_disk_index uint8 100M.bin 100m 96 64 3.3 128 112 l2 1
 build_disk_index uint8 100M.bin 100m 96 64 3.3 32 16 l2 0  
export INDEX_PREFIX=100m 
export DATA_PATH=100M.bin
gen_random_slice uint8 ${DATA_PATH} ${INDEX_PREFIX}_SAMPLE_RATE_0.01 0.01
build_memory_index uint8 ${INDEX_PREFIX}_SAMPLE_RATE_0.01_data.bin ${INDEX_PREFIX}_SAMPLE_RATE_0.01_ids.bin ${INDEX_PREFIX}_mem.index 0 0 32 64 1.2 24 l2
#build/tests/search_disk_index <data_type> <index_prefix> <nthreads> <I/O pipeline width (max for PipeANN)> <query file> <truth file> <top-K> <similarity> <search_mode (2 for PipeANN)> <L of in-memory index> <Ls for on-disk index>
search_disk_index uint8 ${INDEX_PREFIX} 1 32 bigann_query.bbin 100M_gt.bin 10 l2 2 10 10 20 30 40    éœ€è¦æ³¨æ„10æ˜¯topkï¼Œåé¢çš„10ï¼Œ20ï¼Œ30ï¼Œ40ä»£è¡¨å†…å­˜æœç´¢èŠ‚ç‚¹çš„ä¸ªæ•°ï¼Œä¹Ÿå°±æ˜¯æ·±åº¦memlã€‚ 10ï¼Œ20ï¼Œ30ï¼Œ40æ˜¯4ä¸ªæµ‹è¯•ã€‚ æµ‹è¯•ä»£ç ä¼šè·³è¿‡å°äºtopkçš„memlæµ‹è¯•ã€‚


search-insert
å°†ç¬¬äºŒä¸ª 100M å‘é‡æ’å…¥åˆ°ä½¿ç”¨æ•°æ®é›†ä¸­å‰ 100M å‘é‡æ„å»ºçš„ç´¢å¼•ä¸­ï¼Œå¹¶å‘æœç´¢ã€‚
../../PipeANN/build/tests/utils/compute_groundtruth uint8 bigann.bin bigann_query.bbin 1000 truth.bin
build/tests/gt_update truth.bin 200000000 1000000 10 1B_topk 1
æ¯æ’å…¥/åˆ é™¤ 1M ä¸ªè½½ä½“åè®¡ç®—å¬å›ç‡ã€‚


change_pts uint8 bigann.bin 2000000
mv bigann.bin2000000  2M.bin
../../PipeANN/build/tests/utils/compute_groundtruth uint8 2M.bin bigann_query.bbin 1000 2M_truth.bin

 ../../../PipeANN/build/tests/gt_update -h
Correct usage: ../../../PipeANN/build/tests/gt_update <file> <tot_npts> <batch_npts> <target_topk> <target_dir> <insert_only>
../../PipeANN/build/tests/gt_update 2M_truth.bin 2000000 10000 10 2M_topk 1
../../PipeANN/build/tests/test_insert_search uint8 2M.bin 64 10000 1 10 32 0 1m  bigann_query.bbin 2M_topk 0 10 4 4 0 20



search-insert-delete
æ’å…¥ç¬¬äºŒä¸ª 100M å‘é‡å¹¶åˆ é™¤æ•°æ®é›†ä¸­çš„å‰ 100M å‘é‡ï¼Œå¹¶å‘æœç´¢ã€‚
æ¯æ’å…¥/åˆ é™¤ 1M ä¸ªè½½ä½“åè®¡ç®—å¬å›ç‡ã€‚


change_pts uint8 bigann.bin 2000000
mv bigann.bin2000000  2M.bin
../../PipeANN/build/tests/utils/compute_groundtruth uint8 2M.bin bigann_query.bbin 1000 2M_truth.bin
../../PipeANN/build/tests/gt_update 2M_truth.bin  2000000 10000 10 2M_topk 1 (è¿™é‡Œä¸èƒ½äº›insert_only=0ï¼Œæœ‰bugï¼Œè§£æå®Œäº†ä¸€æ¡æ•°æ®éƒ½æ²¡æœ‰)
../../PipeANN/build/tests/overall_performance uint8 2M.bin 128 1m bigann_query.bbin 2M_topk 10 4 100 20 30 




pip install numpy pandas faiss-cpu pyarrow -i https://pypi.tuna.tsinghua.edu.cn/simple

 python ./conver_npy.py
 python convert_to_vdb_format.py   --train train_vectors.npy   --test test_vectors.npy   --out custom_dataset   --topk 100
 scp custom_dataset/ root@10.100.10.221://tmp/vectordb_bench/dataset/














æ ¼å¼è½¬æ¢è„šæœ¬-hdf5åˆ°parquet
annbenchmarkåˆ°vectordbbenchçš„æ ¼å¼
cat conver_npy.py
import h5py
import numpy as np

# ä¸‹è½½å¹¶åŠ è½½ HDF5 æ–‡ä»¶
with h5py.File('sift-128-euclidean.hdf5', 'r') as f:
    train_vectors = f['train'][:]  # è®­ç»ƒå‘é‡ï¼Œå½¢çŠ¶ (num_samples, 128)
    test_vectors = f['test'][:]    # æµ‹è¯•å‘é‡
    neighbors = f['neighbors'][:] # çœŸå®è¿‘é‚» ID åˆ—è¡¨

np.save("train_vectors.npy", train_vectors)
np.save("test_vectors.npy", test_vectors)





cat convert_to_vdb_format.py
import os
import argparse
import numpy as np
import pandas as pd
import faiss
from ast import literal_eval
from typing import Optional
def load_csv(path: str):
    df = pd.read_csv(path)
    if 'emb' not in df.columns:
        raise ValueError(f"CSV æ–‡ä»¶ä¸­ç¼ºå°‘ 'emb' åˆ—ï¼š{path}")
    df['emb'] = df['emb'].apply(literal_eval)
    if 'id' not in df.columns:
        df.insert(0, 'id', range(len(df)))
    return df
def load_npy(path: str):
    arr = np.load(path)
    df = pd.DataFrame({
        'id': range(arr.shape[0]),
        'emb': arr.tolist()
    })
    return df
def load_vectors(path: str) -> pd.DataFrame:
    if path.endswith('.csv'):
        return load_csv(path)
    elif path.endswith('.npy'):
        return load_npy(path)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path}")
def compute_ground_truth(train_vectors: np.ndarray, test_vectors: np.ndarray, top_k: int = 10):
    dim = train_vectors.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(train_vectors)
    _, indices = index.search(test_vectors, top_k)
    return indices
def save_ground_truth(df_path: str, indices: np.ndarray):
    df = pd.DataFrame({
        "id": np.arange(indices.shape[0]),
        "neighbors_id": indices.tolist()
    })
    df.to_parquet(df_path, index=False)
    print(f"âœ… Ground truth ä¿å­˜æˆåŠŸ: {df_path}")
def main(train_path: str, test_path: str, output_dir: str,
         label_path: Optional[str] = None, top_k: int = 10):
    os.makedirs(output_dir, exist_ok=True)
    # åŠ è½½è®­ç»ƒå’ŒæŸ¥è¯¢æ•°æ®
    print("ğŸ“¥ åŠ è½½è®­ç»ƒæ•°æ®...")
    train_df = load_vectors(train_path)
    print("ğŸ“¥ åŠ è½½æŸ¥è¯¢æ•°æ®...")
    test_df = load_vectors(test_path)
    # å‘é‡æå–å¹¶è½¬æ¢ä¸º numpy
    train_vectors = np.array(train_df['emb'].to_list(), dtype='float32')
    test_vectors = np.array(test_df['emb'].to_list(), dtype='float32')
    # ä¿å­˜ä¿ç•™æ‰€æœ‰å­—æ®µçš„ parquet æ–‡ä»¶
    train_df.to_parquet(os.path.join(output_dir, 'train.parquet'), index=False)
    print(f"âœ… train.parquet ä¿å­˜æˆåŠŸï¼Œå…± {len(train_df)} æ¡è®°å½•")
    test_df.to_parquet(os.path.join(output_dir, 'test.parquet'), index=False)
    print(f"âœ… test.parquet ä¿å­˜æˆåŠŸï¼Œå…± {len(test_df)} æ¡è®°å½•")
    # è®¡ç®— ground truth
    print("ğŸ” è®¡ç®— Ground Truthï¼ˆæœ€è¿‘é‚»ï¼‰...")
    gt_indices = compute_ground_truth(train_vectors, test_vectors, top_k=top_k)
    save_ground_truth(os.path.join(output_dir, 'neighbors.parquet'), gt_indices)
    # åŠ è½½å¹¶ä¿å­˜æ ‡ç­¾æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
    if label_path:
        print("ğŸ“¥ åŠ è½½æ ‡ç­¾æ–‡ä»¶...")
        label_df = pd.read_csv(label_path)
        if 'labels' not in label_df.columns:
            raise ValueError("æ ‡ç­¾æ–‡ä»¶ä¸­å¿…é¡»åŒ…å« 'labels' åˆ—")
        label_df['labels'] = label_df['labels'].apply(literal_eval)
        label_df.to_parquet(os.path.join(output_dir, 'scalar_labels.parquet'), index=False)
        print("âœ… æ ‡ç­¾æ–‡ä»¶å·²ä¿å­˜ä¸º scalar_labels.parquet")
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å°†CSV/NPYå‘é‡è½¬æ¢ä¸ºVectorDBBenchæ•°æ®æ ¼å¼ (ä¿ç•™æ‰€æœ‰åˆ—)")
    parser.add_argument("--train", required=True, help="è®­ç»ƒæ•°æ®è·¯å¾„ï¼ˆCSV æˆ– NPYï¼‰")
    parser.add_argument("--test", required=True, help="æŸ¥è¯¢æ•°æ®è·¯å¾„ï¼ˆCSV æˆ– NPYï¼‰")
    parser.add_argument("--out", required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--labels", help="æ ‡ç­¾CSVè·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--topk", type=int, default=10, help="Ground truth")
    args = parser.parse_args()
    main(args.train, args.test, args.out, args.labels, args.topk)

æ ¼å¼è½¬æ¢è„šæœ¬-768M parquentåˆ°fbin
Cohere 1M vectors, 768 dimension
 cat parquet_to_fbin.py
import pandas as pd
import numpy as np
import os
import sys

def save_fbin(path, data):
    """å°†å‘é‡åˆ—è¡¨ä¿å­˜ä¸ºDiskANNå…¼å®¹çš„.fbinæ ¼å¼"""
    with open(path, "wb") as f:
        # å†™å…¥æ–‡ä»¶å¤´ï¼šå‘é‡æ•°é‡å’Œç»´åº¦
        f.write(np.array([len(data), len(data[0])], dtype=np.int32).tobytes())
        # å†™å…¥å‘é‡æ•°æ®
        f.write(np.vstack(data).astype(np.float32).tobytes())

def main():
    parquet_path = "shuffle_train.parquet"

    # 1. è¯»å–Parquetæ–‡ä»¶å¹¶æå–å‘é‡
    print(f"è¯»å–Parquetæ–‡ä»¶: {parquet_path}")
    df = pd.read_parquet(parquet_path)
    print(f"æ£€æµ‹åˆ°åˆ—å: {df.columns.tolist()}")

    # æ ¹æ®å®é™…åˆ—åæå–å‘é‡
    vector_column = 'emb'  # æ ¹æ®å®é™…åˆ—åè°ƒæ•´
    vectors = df[vector_column].apply(np.array).values

    # 2. éªŒè¯ç»´åº¦ä¸€è‡´æ€§
    dim = len(vectors[0])
    assert all(len(v) == dim for v in vectors), f"å‘é‡ç»´åº¦ä¸ä¸€è‡´ï¼åº”ä¸º{dim}ç»´"
    print(f"æ•°æ®éªŒè¯: å…±{len(vectors)}æ¡{dim}ç»´å‘é‡")

    # 3. ä¿å­˜åŸºç¡€é›† (Base)
    base_path = "cohere_base.fbin"
    with open(base_path, "wb") as f:
        f.write(np.array([len(vectors), dim], dtype=np.int32).tobytes())
        f.write(np.vstack(vectors).astype(np.float32).tobytes())
    print(f"å·²ç”ŸæˆåŸºç¡€é›†: {base_path}")

    # 4. éšæœºåˆ’åˆ†Learné›†å’ŒQueryé›† (90% : 10%)
    print("éšæœºåˆ’åˆ†æ•°æ®é›†...")
    indices = np.random.permutation(len(vectors))
    split_idx = int(len(vectors) * 0.9)  # 90%ä½œä¸ºLearné›†

    learn_vectors = [vectors[i] for i in indices[:split_idx]]
    query_vectors = [vectors[i] for i in indices[split_idx:]]

    # 5. ä¿å­˜Learné›†å’ŒQueryé›†
    save_fbin("cohere_learn.fbin", learn_vectors)
    save_fbin("cohere_query.fbin", query_vectors)
    print(f"å·²ç”Ÿæˆå­¦ä¹ é›†: cohere_learn.fbin ({len(learn_vectors)}æ¡)")
    print(f"å·²ç”ŸæˆæŸ¥è¯¢é›†: cohere_query.fbin ({len(query_vectors)}æ¡)")

if __name__ == "__main__":
    main()

 cohere æ˜¯é€šè¿‡vectordbbenchç•Œé¢é€‰æ‹©çš„ cohere 1M 768å‘é‡ä¸‹è½½çš„ ä¸‹è½½è·¯å¾„ä¸º/tmp/vectordb_bench/dataset/cohere/cohere_medium_1m
 
 cohere_base.fbin  cohere_learn.fbin  cohere_query.fbin  neighbors.parquet  parquet_to_fbin.py  scalar_labels.parquet  shuffle_train.parquet  test.parquet
 
 
 python parquet_to_fbin.py
è¯»å–Parquetæ–‡ä»¶: shuffle_train.parquet
æ£€æµ‹åˆ°åˆ—å: ['id', 'emb']
æ•°æ®éªŒè¯: å…±1000000æ¡768ç»´å‘é‡
å·²ç”ŸæˆåŸºç¡€é›†: cohere_base.fbin
éšæœºåˆ’åˆ†æ•°æ®é›†...
å·²ç”Ÿæˆå­¦ä¹ é›†: cohere_learn.fbin (900000æ¡)
å·²ç”ŸæˆæŸ¥è¯¢é›†: cohere_query.fbin (100000æ¡)


 ./build/apps/utils/compute_groundtruth  --data_type float --dist_fn l2 --base_file ../dataset/cohere/cohere_learn.fbin --query_file  ../dataset/cohere/cohere_query.fbin --gt_file ../dataset/cohere/cohere_query_learn_gt100 --K 100
        Going to compute 100 NNs for 100000 queries over 900000 points in 768 dimensions using L2 distance fn.
        
     2.9G    cohere_base.fbin
2.6G    cohere_learn.fbin
293M    cohere_query.fbin
77M     cohere_query_learn_gt100



vectordbbenchéƒ¨ç½²
https://github.com/milvus-io/milvus/blob/master/configs/milvus.yamlçš„é…ç½®æ–‡ä»¶




python run.py --generate --engine infinity --dataset enwiki
python run.py --import --engine infinity --dataset enwiki
python run.py --query=16 --engine infinity --dataset enwiki
python run.py --query-express=16 --engine infinity --dataset enwiki



